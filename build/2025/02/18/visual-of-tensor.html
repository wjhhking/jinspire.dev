<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jinspire</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
     
</head>
<body>
    <main>
        <header>
            <h1>Jinspire</h1>
            <p>Writing about curiosity, insights, and reflection.</p>
            <nav class="nav-container">
                <a href="/" class="nav-link">â† Back</a>
            </nav>
        </header>

        <article class="content">
    <h1>Visualize Generation in LLM</h1>
    <span class="date">
        
            February 18, 2025
        
    </span>

    <!-- æ·»åŠ TOCåˆ‡æ¢æŒ‰é’® - æ ¹æ®è¯­è¨€æ˜¾ç¤ºä¸åŒæ–‡æœ¬ -->
    <button class="toc-toggle">
        Catalog
    </button>

    <!-- æ·»åŠ TOCå®¹å™¨ -->
    <div class="toc-container" style="display: none;">
        <h4>Catalog</h4>
        <ul class="toc-list" id="toc"></ul>
    </div>

    <div class="post-content">
        
<h1 id="the_capital_of_france_is____">The capital of France is ___</h1>

<p>When a large model generates a new word, how do the matrices inside the model change?</p>

<h1 id="matrix_multiplication">Matrix Multiplication</h1>

<p>Letâ€™s visualize matrix multiplication first. Note how the dimensions of the two matrices are matched, we will use this pattern in below visualizations.</p>

<p><img src="/assets/images/posts/mat_mul.png" alt="mat_mul" /></p>

<p>Blue: Data (Tensor)</p>

<p>Grey: Model Weights</p>

<h1 id="embedding">Embedding</h1>

<p><img src="/assets/images/posts/embedding_layer.png" alt="embedding_layer" /></p>

<p>Start with the embedding layer. Assume one word equals one token.</p>

<p>Input sentence: The capital of France is ___</p>

<p>-&gt; [The, capital, of, France, is]</p>

<p>-&gt; [464, 3361, 295, 2238, 318], which are their Token IDs</p>

<p>This forms a [1, 5] vector.</p>

<p>The data now enters an embedding table of size [50,000, 768] to look up: - The = [0.316, 0.524, 0.063, â€¦] - capital = [0.123, 0.234, 0.345, â€¦] - of = [0.432, 0.543, 0.654, â€¦] - France = [0.543, 0.654, 0.765, â€¦] - is = [0.654, 0.765, 0.876, â€¦]</p>

<p>-&gt; Thus, we obtain a [5, 768] matrix!</p>

<p>We pad (appending 0s) it to length 1024, which now becomes [1024, 768].</p>

<h1 id="transformer_block">Transformer Block</h1>

<p><img src="/assets/images/posts/transformer_block.png" alt="transformer_block" /></p>

<p>Three key matrices: Q, K, V. Each is [768, 64] * 12.</p>

<p>For one head, [1024, 768] * [768, 64] = [1024, 64]</p>

<p>Q 12 heads = [1024, 64] + [1024, 64] + â€¦ + [1024, 64]</p>

<p>= [1024, 64] * 12 = [1024, 768]</p>

<p>K and V are the same.</p>

<p>K^T = [768, 1024]</p>

<p>QK^T = [1024, 1024]</p>

<p>Softmax(QK^T/âˆšd) = [1024, 1024]</p>

<p>Attn = Softmax(QK^T/âˆšd) * V = [1024, 768]</p>

<p>MLP has two layers: one expands 4x using [768 Ã— 3072] and one contracts using [3072 Ã— 768]</p>

<p>Change: [1024, 768] -&gt; [1024, 3072] -&gt; [1024, 768]</p>

<h1 id="the_whole_process">The Whole Process</h1>

<p><img src="/assets/images/posts/transformer_tensors.png" alt="transformer_tensors" /></p>

<p>Extract the last row: [768]</p>

<p>Then, use the original embedding table [50,000, 768] to map back to a Token ID, retrieving the final word: Paris</p>

<h1 id="parameters_of_models">Parameters of Models</h1>

<p>All the transformer models have a similar structure, just different numbers of layers(12), d_model(768), d_head(64), etc. The appendix shows the number of parameters for the famous models.</p>

<p>Latest research aims to reduce the total number of parameters using different attention mechanisms (e.g., MLA, NSA), which is not the focus of this article.</p>

<p>Thank you for reading! ğŸ‰ ğŸ¥° ğŸ«¡</p>

<h1 id="appendix_number_of_parameters">Appendix: Number of Parameters</h1>

<p><em>Note below tables are assisted by AI, and not manually verified carefully.</em></p>

<h3 id="llama_family">Llama Family</h3>

<p><img src="/assets/images/posts/llama_parameters.png" alt="llama_parameters" /></p>

<h3 id="qwen_family">Qwen Family</h3>

<p><img src="/assets/images/posts/qwen_parameters.png" alt="qwen_parameters" /></p>

<h3 id="deepseek_family">DeepSeek Family</h3>

<p><img src="/assets/images/posts/deepseek_parameters.png" alt="deepseek_parameters" /></p>

<h3 id="chatgpt_family">ChatGPT Family</h3>

<p><img src="/assets/images/posts/chatgpt_parameters.png" alt="chatgpt_parameters" /></p>

<h3 id="other">Other</h3>

<p>Given Claude and Gemini are closed source, it does not provide much value just showing the guesses. Same for Grok, Grok-3 just got released. Letâ€™s wait and see!</p>

    </div>
</article>

<!-- æ·»åŠ TOCç”Ÿæˆå’Œé«˜äº®è„šæœ¬ -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    // è·å–æ‰€æœ‰æ ‡é¢˜å…ƒç´ 
    const headings = document.querySelectorAll('.post-content h1, .post-content h2, .post-content h3, .post-content h4');
    const toc = document.getElementById('toc');
    const tocContainer = document.querySelector('.toc-container');
    const tocToggle = document.querySelector('.toc-toggle');
    const isChinesePage = false;

    // å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä¸æ˜¾ç¤ºTOC
    if (headings.length === 0) {
        tocToggle.style.display = 'none';
        return;
    }

    // åˆ‡æ¢ç›®å½•æ˜¾ç¤º
    tocToggle.addEventListener('click', function() {
        if (tocContainer.style.display === 'none') {
            tocContainer.style.display = 'block';
            tocToggle.textContent = isChinesePage ? 'å…³é—­ç›®å½•' : 'Close Catalog';
        } else {
            tocContainer.style.display = 'none';
            tocToggle.textContent = isChinesePage ? 'ç›®å½•' : 'Catalog';
        }
    });

    // ç”Ÿæˆç›®å½•
    headings.forEach(function(heading, index) {
        // ä¸ºæ¯ä¸ªæ ‡é¢˜åˆ›å»ºID
        if (!heading.id) {
            heading.id = 'heading-' + index;
        }

        const li = document.createElement('li');
        const a = document.createElement('a');

        a.href = '#' + heading.id;
        a.textContent = heading.textContent;
        a.classList.add('toc-' + heading.tagName.toLowerCase());

        li.appendChild(a);
        toc.appendChild(li);
    });

    // ç›‘å¬æ»šåŠ¨ï¼Œé«˜äº®å½“å‰æ ‡é¢˜
    const tocLinks = document.querySelectorAll('.toc-list a');

    function highlightToc() {
        let scrollPosition = window.scrollY;

        // æ‰¾åˆ°å½“å‰å¯è§çš„æ ‡é¢˜
        let currentHeading = null;

        for (let i = 0; i < headings.length; i++) {
            const heading = headings[i];
            const rect = heading.getBoundingClientRect();

            // æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨è§†å£ä¸­æˆ–åˆšåˆšè¶…å‡ºé¡¶éƒ¨
            if (rect.top <= 100) {
                currentHeading = heading;
            } else {
                break;
            }
        }

        // ç§»é™¤æ‰€æœ‰activeç±»
        tocLinks.forEach(link => link.classList.remove('active'));

        // å¦‚æœæ‰¾åˆ°å½“å‰æ ‡é¢˜ï¼Œé«˜äº®å¯¹åº”çš„TOCé“¾æ¥
        if (currentHeading) {
            const currentLink = document.querySelector(`.toc-list a[href="#${currentHeading.id}"]`);
            if (currentLink) {
                currentLink.classList.add('active');
            }
        }
    }

    // åˆå§‹åŒ–é«˜äº®
    highlightToc();

    // ç›‘å¬æ»šåŠ¨äº‹ä»¶
    window.addEventListener('scroll', highlightToc);
});
</script>

        <footer>
            <p>Â© 2025 Jinspire Â· <a href="mailto:jinwu76@gmail.com">Email</a></p>
        </footer>
    </main>

    
</body>
</html>