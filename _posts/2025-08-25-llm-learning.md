---
layout: post
title: "What are some practical implications of LLM training for my own learning process?"
date: 2025-08-25
lang: en
published: true
---

I recently learned more about how large language models (LLMs) are trained, and at the same time, I began to notice parallels with my own learning process as a human. The more I thought about it, the more it seemed that the stages of LLM training mirror how we grow, work, and live.

Here are some of the most practical insights I found:

- **Pretraining is like schooling.** It is foundational. We should keep “schooling” or pretraining ourselves, regardless of age.
- **Supervised finetuning (SFT) is like working.** In a job, we slightly adjust ourselves to prefer certain outputs (behaviors, deliverables) over others.
- **Reinforcement learning (RL) happens all the time.** We are constantly being “RLed by the world.” Rewards like dopamine hits from TikTok doomscrolling keep us hooked, while the lack of immediate rewards for saving money or exercising makes them harder to sustain.

So, what are some practical implications for our own lives?

- **Encoder alone is not enough.** Input matters, but output is crucial too. Like Feynman’s learning method, we should teach and explain to others as a way to consolidate knowledge.
- **We should keep pretraining ourselves.** Continuous education and curiosity shouldn’t stop at graduation.
- **We need to set the right reward model for ourselves.** Otherwise, the world sets it for us — often misaligned with our long-term goals.

---

## A Few New Thoughts

Some time ago, I ran a thought experiment:

```
Thought experiment: If humans were LLMs…

- Inventing language → Tokenization
- Taking notes → RAG & context retrieval
- Going to school → Knowledge distillation
- Empathy → Reading hidden states / latent embeddings
- Dividing subjects → Mixture of Experts (MoE)
- Creating new subjects → New tasks, loss functions, and evaluation metrics

There are many more analogies from different perspectives—an interesting lens to explore!
```

Recently, I’ve been extending this analogy further:

- Pretraining is like schooling.
- Working is like SFT.
- Everyday life is constant RL.
- Hallucination is like lying or dreaming.
- MoE is more like brain regions than school subjects.
- A model looping the same output is like a person getting high and repeating themselves endlessly.

This lens is not perfect, but it makes us reflect: **if we are like LLMs, how can we train ourselves better?**

---

## Pretraining

Most of school life is about learning and taking exams — more or less like predicting or selecting the next token.

At first, it feels like rote memorization. But over time, patterns emerge and knowledge crystallizes, allowing us to master a subject and generalize beyond it.

AI researcher Denny Zhou has argued that reasoning ability already emerges during pretraining. If it’s not in the pretrained base, it’s hard to acquire later. Humans are the same: without a broad foundation, it is difficult to pick up deep reasoning skills later in life.

That’s why pretraining is so important. In society, we even see a bias toward certain schools — similar to choosing certain base models to build on.

---

## Supervised Finetuning (SFT)

The work environment is like SFT.

Even if our base models (educational backgrounds) differ, the process of being shaped by professional expectations can change us dramatically. A workplace rewards certain responses over others — just like supervised finetuning. Over time, we learn to produce the outputs that are expected, valued, and rewarded.

This also suggests we should **finetune intentionally**: choose environments, mentors, and projects that push us in the direction we actually want to grow.

---

## Constant RL from the World

The most important thing in RL is the **reward function**.

- Pavlov famously trained dogs through rewards.
- Charlie Munger (Buffett’s longtime partner) put it bluntly:
  > *“Show me the incentive, and I will show you the outcome.”*
- Or in his words: *“Never, ever, think about something else when you should be thinking about the power of incentives.”*

**Reward matters.**

We are constantly being RLed by the world.
- Short-term: dopamine hits from TikTok, likes on social media, the thrill of a purchase.
- Long-term: habits like saving money, exercising, or building a skill — which are harder to sustain because rewards are delayed.

If our personal “reward model” is misaligned, we optimize for short-term gratification at the expense of long-term flourishing. This is reward hacking, human-style.

So how do we fix it?
- **Gamify learning.** I’ve been experimenting with “additive learning”: giving myself points and badges for completing tasks, then trading those points for rewards (like phone time). It’s simple, but it works.
- **Identity as reinforcement.** If you see yourself as a “lifelong learner,” then learning feels less like effort and more like alignment with your identity.
- **On-demand learning.** Richard Sutton, one of the fathers of RL, proposed in his recent [“Oak” talk](https://www.youtube.com/watch?v=gEbbGyNkR2U) that we should train directly for what is being used, instead of separating pretraining, SFT, and RL. This applies even more to humans: we learn best when the reward is tied directly to real-world use — what Andrej Karpathy calls “on-demand learning.”

---

## What Are the Implications for Ourselves?

Bringing it all together:

- **Pretrain endlessly.** Keep exposing yourself to new information, ideas, and perspectives — regardless of age.
- **Output, don’t just learn (encode).** Teach, write, explain — that’s how knowledge sticks.
- **Be conscious of your reward model.** If you don’t set it, the world sets it for you. And the world’s incentives (likes, clicks, quick dopamine) are rarely aligned with your deepest goals.
- **Finetune intentionally.** Don’t just let work or culture shape you passively. Choose environments that reinforce what you want to become.
- **Learn continually and on-demand.** Shift from “learn first, then do” to “learn while doing.” Treat every task as a chance to update your model weights.

I recently developed an app called "Human Pretraining" and I look forward to sharing it with you soon!