---
layout: post
title: "What I learned from 100 hours into reinforcement learning"
date: 2025-07-20
lang: en
published: false
---


---
layout: post
title: "What are some practical implications of LLM learning for ourselves?"
date: 2025-8-25
lang: en
published: false
---


I recentely learned more about LLM training, and self-observed a lot of things about my learning process.

- Pretraining is like schooling, it is very important. We should keep schooling / pretraining our selfies, independe of the age.
- Supervised finetuning(SFT) is like working, we prefer certain outputs over others, we we slightly modify our brain to generate certain outputs.
- We are constantly being RLed by the world. We get instant rewards, aka dopamine hits, for watching tiktok, so we keep doomscrolling, which we need to correct.

What are a few practical implications for ourselves?

- Encoder itself is not enough. we need to output - like Ferman's learning method to teach others.
- We should keep schooling / pretraining our selfies, independe of the age.
- We should set correct reward model for ourselves.






## A few new thoughts

I had a thought experiment a while back,

```
Thought experiment: If humans were LLMs…

- Inventing language → Tokenization
- Taking notes → RAG & context retrieval
- Going to school → Knowledge distillation
- Empathy → Reading hidden states / latent embeddings
- Dividing subjects → Mixture of Experts (MoE)
- Creating new subjects → New tasks, loss functions, and evaluation metrics

There are many more analogies from different perspectives—an interesting lens to explore!
```

Recently, I've been having more ideas about this.

For example, pretraining is like schooling. Working is like SFT. We are constently RL in everyday.

Before we dive deeper, I'd like to share a few more analogies.

- Hallucination is like lying in a dream.
- A correction of MoE is that it is more like brain sections instead of subjects.
- models output same content loop is like human getting high and repeating the same thing over and over again.






## pretraining

Most of our school life is about learning and take exams - which is more or less like predicting/selecting the next token.

We memorize a lot of information, then at one point, we emerge and things start to make sense, where we start ace one specfic topic / subject.



There has been an argument by Denny Zhou (https://www.youtube.com/watch?v=ebnX5Ur1hBk) that the reasoing ability is already embedded in the pretraining process.

For a model, if the pretraining does not have certain ability, it is relatively hard to learn it in the later stage.

Pretraining is very important.

In human society, a lot of companies are biased towards certain schools, which is like choosing certain base models.

### SFT

Work enviorment is like SFT.

We may be ahead or behind in the base models, but the SFT process could change a lot of things.



### we are constantly being RLed by the world.


The single most important thing for the RL process is the reward function.

巴普勒夫 trained the dogs, through rewards.

Charlie Munger said "Never, ever, think about something else when you should be thinking about the power of incentives."


reward matters.


We are constantly being RLed by the world.

We get instant rewards, aka dopamine hits, for watching tiktok, so we keep doomscrolling.

We do not get rewards for some long term goals, like saving money, exercising, so it is hard to keep doing them.


We should set correct reward model for ourselves.

One thing I recently read a book called "additive learning" which talks about gamification of learning tasks. Simple hacks, like  points, badges just work for us, not just for kids. So I started again to reward a completion of a learning task with a score and reward for playing cell phone.

Another hack is about self identity 认同，if you identify yourself as a constant learner, learning is less effortful because it aligns with your identity.



In the RL god, Richard Sutton's recent [talk](https://www.youtube.com/watch?v=gEbbGyNkR2U), he proposed "Oak", said about continous learning, and the first principle to train for what is being used, which is more or less different from the current pretraining + SFT + RL scheme. This actually applies to human more than the model, the we should keep learning and modify our brain connections (model weights) when we are working, which is also called "on demand learning" by Andrew Karpathy.


## What are the implications for ourselves?

- Encoder itself is not enough. We need to output - like Ferman's learning method.
-





-----