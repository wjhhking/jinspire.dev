---
layout: post
title: "Machine Learning Resources Index"
date: 2025-05-25
lang: en
---

The main purpose is to list all the resources I've collected about Machine Learning (ML), Artificial Intelligence (AI), and Large Language Models (LLM).

# ML Basics

For a intro to ML, I recommend the following resources:

Courses:
- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)
- [Stanford CS229: Machine Learning by Andrew Ng](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)


Videos:
- [3Blue1Brown's intro to neural network](https://www.youtube.com/watch?v=aircAruvnKk)
  - The youtube channel also has a lot of good videos about math (linear algebra, calculus, etc.), physics, and a lot of science topics.
- [Andrew Karpathy's Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

There are many more excellent introductory materials.


# LLM

### The two best courses for LLM are:

- [Stanford CS224n](https://web.stanford.edu/class/cs224n/) (winter 2025)
  - A very good course for a lot of intuitions for LLM with [Youtube videos](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)
- [Stanford CS336](https://stanford-cs336.github.io/spring2025/) (spring 2025)
  - The most hard-core course for LLM with [Youtube videos](https://www.youtube.com/watch?v=SQ3fZ1sAqXI&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)

### Videos:
- [Chinese: GPT 1-3 paper read by Mu Li](https://youtu.be/t70Bl3w7bxY?si=U5EUnJ5mfV1NN-gP)
  - A famous researcher and youtuber, who has good videos about **Transformer**, **GPT1-4**, **InstructGPT**, **CLIP**, **GAN**, **Whisper**, etc.

- [Google DeepMind: How to Scale Your Model](https://jax-ml.github.io/scaling-book/)

### Training Infra

- [Introduction to LLM Parallelism](https://jinspire.dev/2025/03/09/parallelism101.html)
  - A good introduction to the parallelism in LLM training from myself.
- [The Ultra-Scale Playbook:
Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview)
  - Hugging Face's playbook for training LLMs on GPU clusters. Very comprehensive and detailed.

### Reinforcement Learning

- [RL book from Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf) (2020)
- [Youtube videos from EZ.Encoder](https://www.youtube.com/@ez.encoder.academy)
  - Excellent videos about *Deepseek*, *RL*, etc.


### Interpretability

- [A Mathematical Framework for Transformer Circuits](https://arxiv.org/pdf/2111.03382) (Dec 2021)
  - Anthropic's paper conceptualizing the operation of transformers in a new but mathematically equivalent way and making sense of these small models and gain significant understanding of how they operate internally.
- [In-Context Learning and Induction Heads](https://arxiv.org/pdf/2209.11895) (Mar 2022)
  - Anthropic's paper arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size.
- [Towards Monosemanticity: Decomposing Language Models with Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features) (Oct 2023)
  - Anthropic's paper using a sparse autoencoder to extract a large number of interpretable features from a one-layer transformer.
- [Sparse Autoencoders Find Highly Interpretable Model Directions](https://arxiv.org/abs/2310.10348) (Oct 2023)
  - Using sparse autoencoders to find meaningful directions within a model's activations.
- [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/) (May 2024)
  - Applying and scaling these ideas to larger, more capable models like Claude 3 Sonnet.
- [Scaling and evaluating sparse autoencoders](https://arxiv.org/abs/2406.04093) (Jun 2024)
  - Exploring the practical aspects and effectiveness of sparse autoencoders at scale.
- [Transcoders find interpretable LLM feature circuits](https://arxiv.org/abs/2406.11944) (Jun 2024)
  - Focusing on finding interpretable circuits of features within LLMs.
- [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) (Mar 2025)
  - Anthropic's most recent paper for LLM Interpretability.

Just like you can infinite scroll on TikTok, you can infinite scroll on the papers.

### Agent

- [UCB CS294/194-196 Large Language Model Agents](https://rdi.berkeley.edu/adv-llm-agents/sp25)
  - A good course from UC Berkeley with [Youtube videos](https://www.youtube.com/live/g0Dwtf3BH-0), which invited a lot of frontier researchers to give lectures.
  - List of topics
    - Inference-Time Techniques & Reasoning (CoT, ReAct, RAG, Planning, etc.)
    - Coding Agents
    - Multimodal Autonomous AI Agents
    - AlphaProof, Science Discovery
    - Reinforcement Learning
    - Safety & Vulnerability
    - etc.
  - I'm planning to write a summary for this course.

###

[All About Transformer Inference](https://jax-ml.github.io/scaling-book/inference/)

### Recent LLM Papers (that I read and liked)

Need to mention that a lot of the courses and resources already include a lot of good papers.

- Speculative deconding papers
  - [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) (Nov 2022)
  - [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318) (Feb 2023)
  - [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://arxiv.org/abs/2310.08461) (Oct 2023)
  - [Decoding Speculative Decoding](https://arxiv.org/abs/2402.01528) (Feb 2024)


# Vision

Intro:
- [Image Generation 101: an Introduction](https://jinspire.dev/2025/05/10/image101.html) A good introduction to the image generation from myself.

Courses:
- [Stanfodd CS231n](https://cs231n.stanford.edu/)
   - There seemse to be videos [only from 2017](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

### Papers

- The intro listed a lot of important papers.

- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) (May 2022)
  - The famours "Let's think step by step" paper.


# News & Blogs

- [Lil’Log](https://lilianweng.github.io/)
  - Lilian Weng's blog, ex VP of Research at OpenAI.

- Chinese: I usually listen to [大飞](https://www.youtube.com/@bestpartners)
- Chinese: A good one is [aidaily.win](https://aidaily.win/)
- [AI Weekly](https://www.aiweekly.co/)




