---
layout: post
title: "What I learned from using 100 hours to build an embedding search engine from ground up"
date: 2025-07-15
lang: en
---

I spend 100 hours to build an embedding search engine from ground up, and I ended up learning much more. I want to share what I learned, to save you some time when you need to go through a similar journey. I'll summarize my understanding into a **brief history**, and then the **basic and fundamental concepts**, and then share the **best engineering suggestions and practices** I learned.

TL;DR: 

- I built a semantic search engine from ground up in 100 hours, for search products.
- I classify the search into **3 levels**: **keyword search**, **embedding search**, and **agentic search**.
- Keyword search can not satisfy the need for search a product using a description, where we need **sematic matching** (cloth vs shirt), and even **multimodal information** (images, voice, etc.).
- Embedding, a vector representation of objects (users, products, etc.), is essentially just **a list of numbers, e.g., [0.37, 0.2, 0.48, 0.4]**.
- It basically trying to **condense the information** of objects (a.k.a. reduce the dimensionality of the object description), so we can use distance to measure the similarity between objects. It is a more efficient way than the old "one-hot" encoding.
- Embedding is the **backbone of LLM** (Tokenizater like word2vec, BPE, etc.). It is also the foundation of recommendation and ranking systems (basically a user embedding + item embedding -> similarity score).
- While LLM can deal with negative prompts, object embedding still can NOT deal with negative prompts, e.g., "not red" will be much closer to "red" than "blue".
- Unified formatting helps the accuracy, but structured data (JSON) hurts the performance.
- My recommendated stack: **PostgresSQL** (traditional database) + **Qwen** (text embedding, and optionally reranker) + **CLIP** (multimodal embedding) + **pgvector** (vector database) + **HNSW** (approximate nearest neighbor algorithm) + optionally your search software for multiple rounds (hybrid elastic search, RAG, agentic search, etc.).
- More details in the following sections.


## Brief History of Embedding


The history can be phased as from **keyword search** (index, frequncy, inverted index) to **semantic search** (embedding, vector database, approximate nearest neighbor algorithm), which clearly lead to **hybrid search / RAG (Retrieval-Augmented Generation) / Agentic search (multi-round with LLMs)**.


![History of Major Milestones in Embedding]({{ '/assets/images/posts/embedding101_history.jpg' | relative_url }})

Here are the key milestones in the evolution of search and embedding technology:

<span class="color-grey">**pre-1990: Term Frequency-Inverse Document Frequency (TF-IDF) & BM25**</span>

- Foundational statistical methods that enabled search engines to rank documents based on keyword frequency and relevance, forming the bedrock of lexical search. Kd-tree is another keyword you should know if you are interested in this topic.

- *Reference: Jones, K. S. (1972). A statistical interpretation of term specificity and its application in retrieval.*

<span class="color-grey">**1996: Google PageRank**</span>

- A revolutionary algorithm that determined a webpage's importance by analyzing the quantity and quality of links pointing to it, fundamentally changing web search.

- *[Reference: Page, L., Brin, S., et al. (1998). The Anatomy of a Large-Scale Hypertextual Web Search Engine](https://research.google/pubs/the-anatomy-of-a-large-scale-hypertextual-web-search-engine/).*

<span class="color-grey">**2010: Elasticsearch**</span>

- A distributed search and analytics engine that made scalable, real-time full-text search widely accessible to developers.

- *[Reference: Elasticsearch Project](https://www.elastic.co/elasticsearch).*

<span class="color-blue">**2013: Word2vec**</span>

- A groundbreaking model that learned to represent words as numerical vectors, capturing their semantic relationships and enabling machines to understand context.

- *[Reference: Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781).*

<span class="color-green">**2015: HNSW (Hierarchical Navigable Small Worlds)**</span>

- A highly efficient algorithm for approximate nearest neighbor search, making it practical to find similar items in massive vector datasets almost instantly.

- *Reference: Malkov, Y. A., & Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.*

<span class="color-brown">**2017: Transformer**</span>

- A novel neural network architecture based on a self-attention mechanism, which became the foundational technology for nearly all modern large language models. Probably also need to mention BERT.

- *[Reference: Vaswani, A., et al. (2017). Attention Is All You Need](https://arxiv.org/abs/1706.03762).*
- *[Reference: Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).*

<span class="color-blue">**2017: FAISS (Facebook AI Similarity Search)**</span>

- A core library developed by Meta (Facebook) AI providing highly optimized algorithms for efficient similarity search and clustering of dense vectors.

- *[Reference: Johnson, J., Douze, M., & Jégou, H. (2017). Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734).*

<span class="color-green">**2019: pgvector**</span>

- An open-source extension for the PostgreSQL database that added vector similarity search, allowing developers to combine traditional and semantic search in one system. 

- *[Reference: pgvector GitHub Repository](https://github.com/pgvector/pgvector).*

<span class="color-green">**2021: Pinecone**</span>

- Established in 2019 and get public attention in 2021, Pinecone is a fully managed, cloud-native vector database designed for high-performance, large-scale similarity search in production applications. Basically a market leader for vector database, although everyone has basically caught up.

- *[Reference: Pinecone Website](https://www.pinecone.io/).*

<span class="color-blue">**2021: CLIP (Contrastive Language–Image Pre-training)**</span>

- A multi-modal model from OpenAI that connects images and text by learning from their natural language descriptions, enabling powerful cross-modal search. There is an open source version which is **still the state-of-the-art for image embedding** (LAION-5B bigG).

- *[Reference: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020).*
- *[Reference: Lai CLIP](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k). downloaded 2 million times.*

<span class="color-brown">**2022: ChatGPT Moment**</span>

- The public release of ChatGPT, which made advanced AI accessible to millions and triggered a massive wave of development in generative AI.

- *Reference: OpenAI Blog Post for ChatGPT.*

<span class="color-blue">**2022+: Foundation Embedding Models**</span> & <span class="color-brown">**LLMs**</span>

- The rapid advancement and proliferation of massive neural networks and specialized models (e.g., from OpenAI, Cohere) for generating text and creating high-quality vector embeddings.

<span class="color-brown">**2023: RAG (Retrieval-Augmented Generation)**</span>

- While proposed in 2020, RAG became popular in 2023 with the release of GPT-3.5 and GPT-4. It is a powerful architecture that enhances LLMs by first retrieving relevant data from an external knowledge base (like a vector database) to generate more accurate answers.

- *Reference: Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.*


## Basic Concepts

What is embedding?

- A vector representation of objects (users, products, etc.).
- It is essentially just a list of numbers, e.g., [0.37, 0.2, 0.48, 0.4].
- It basically trying to condense the information of objects (a.k.a. reduce the dimensionality of the object description), so we can use distance to measure the similarity between objects. It is a more efficient way than the old "one-hot" encoding.
- It is the backbone of LLM (Tokenizater like word2vec, BPE, etc.). It is also the foundation of recommendation and ranking systems (basically a user embedding + item embedding -> similarity score).
- While LLM can deal with negative prompts, object embedding still can NOT deal with negative prompts, e.g., "not red" will be much closer to "red" than "blue".

Cosine similarity vs Euclidean distance (L2 norm)

- Cosine similarity is a measure of similarity between two vectors, which is the cosine of the angle between them.
- Euclidean distance is a measure of similarity between two vectors, which is the square root of the sum of the squared differences between the two vectors.
- Cosine similarity is more sensitive to the angle between the two vectors, while Euclidean distance is more sensitive to the magnitude of the two vectors.
- Default to cosine similarity for embedding search.



## Model Selection

- [Qwen 3](https://huggingface.co/collections/Qwen/qwen3-embedding-6841b2055b99c44d9a4c371f) is king, open source with good size selections. They also provide you a [reranker model](https://huggingface.co/collections/Qwen/qwen3-reranker-6841b22d0192d7ade9cdefea), which gives you a pairwise score if your need accuracy.
- `all-mpnet-base-v2` was the old king, but not anymore.
- OpenAI is still a good option, and a default option for a lot of developers because of their early adoption.
- Cohere is the startup market leader for embedding models. There is a story of Nils Reimers, who was the author of the sentence-transformer, joined Hugging Face, and then Cohere.
- Vovaybe is another good option, which is acquired by MongoDB (2 years into 200 million) and I happen to know the CEO Tengyu Ma, a Stanford professor.
- Gemini released an [embedding model](https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/) July 2025, which was actually already released in March 2025. They had terrible documentation which confused me but here is the [verified (July 2025) way to use it](https://ai.google.dev/gemini-api/docs/models#gemini-embedding). Also you can deploy your own embedding models using Vertex AI.
- [MTEB embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard) is your best friend.
- For multimodal embedding, you can use CLIP, sigCLIP.
- I didn't went deep, but supabase, llamaindex, milvus, qdrant, can be good options as well.


## Vector Databases 


## Chunking

I didn't encounter this issues, but it it common if you have a really a big file.

## A classic Two-Stage Retrieval Process: Recall + Rerank

You can add another middle layer, pre-rank, which is basically a simpler version of reranker.

## Future of search

Agentic search is definitely the future.

Some people call it deep search.

Some people call it AI for science.









